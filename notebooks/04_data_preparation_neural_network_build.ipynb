{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "path_to_py = os.path.abspath('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (189457, 23)\n",
      "\n",
      "Feature names: ['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'purpose', 'percent_bc_gt_75', 'bc_util', 'dti', 'inq_last_6mths', 'mths_since_recent_inq', 'revol_util', 'total_bc_limit', 'mths_since_last_major_derog', 'tot_hi_cred_lim', 'tot_cur_bal', 'internal_score', 'bad_flag', 'no_credit_card_history', 'no_derog_history', 'credit_limit_group', 'emp_length_missing']\n",
      "\n",
      "Data types:\n",
      " loan_amnt                        int64\n",
      "term                            object\n",
      "int_rate                       float64\n",
      "emp_length                     float64\n",
      "home_ownership                  object\n",
      "annual_inc                     float64\n",
      "purpose                         object\n",
      "percent_bc_gt_75               float64\n",
      "bc_util                        float64\n",
      "dti                            float64\n",
      "inq_last_6mths                 float64\n",
      "mths_since_recent_inq          float64\n",
      "revol_util                     float64\n",
      "total_bc_limit                 float64\n",
      "mths_since_last_major_derog    float64\n",
      "tot_hi_cred_lim                float64\n",
      "tot_cur_bal                    float64\n",
      "internal_score                   int64\n",
      "bad_flag                       float64\n",
      "no_credit_card_history           int64\n",
      "no_derog_history                 int64\n",
      "credit_limit_group              object\n",
      "emp_length_missing               int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv(os.path.join(path_to_py, \"data\", \"cleaned_train.csv\"))\n",
    "\n",
    "# Initial check\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFeature names:\", df.columns.tolist())\n",
    "print(\"\\nData types:\\n\", df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Feature Encoding\n",
    "\n",
    "Based on our EDA:\n",
    "- Home ownership: Ordinal encoding based on default rates\n",
    "    - MORTGAGE (5.83%) -> 0\n",
    "    - OWN (7%) -> 1\n",
    "    - RENT (8.4%) -> 2\n",
    "    - NONE (9.5%) -> 3\n",
    "    - OTHER (10.87%) -> 4\n",
    "\n",
    "- Loan Purpose: Risk-based groupings then encode\n",
    "    - Low risk (credit_card, home_improvement) -> 0\n",
    "    - Medium risk (debt_consolidation, car, major_purchase) -> 1\n",
    "    - High risk (small_business, renewable_energy) -> 2\n",
    "\n",
    "- Credit Limit Group: Ordinal encoding (already ordered Q1-Q5 by risk)\n",
    "    - Q5 (4.73%) -> 0\n",
    "    - Q4 -> 1\n",
    "    - Q3 -> 2\n",
    "    - Q2 -> 3\n",
    "    - Q1 (9.87%) -> 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded features check:\n",
      "\n",
      "home_ownership_encoded value counts:\n",
      "home_ownership_encoded\n",
      "0    97647\n",
      "1    15573\n",
      "2    76149\n",
      "3       42\n",
      "4       46\n",
      "Name: count, dtype: int64\n",
      "\n",
      "credit_limit_encoded value counts:\n",
      "credit_limit_encoded\n",
      "0    37892\n",
      "1    37891\n",
      "2    37891\n",
      "3    37891\n",
      "4    37892\n",
      "Name: count, dtype: int64\n",
      "\n",
      "purpose_encoded value counts:\n",
      "purpose_encoded\n",
      "0     59436\n",
      "1    118194\n",
      "2     11827\n",
      "Name: count, dtype: int64\n",
      "\n",
      "term_encoded value counts:\n",
      "term_encoded\n",
      "0    144800\n",
      "1     44657\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updated dataset shape: (189457, 23)\n"
     ]
    }
   ],
   "source": [
    "# Home ownership encoding based on default rates (ascending order of risk)\n",
    "home_ownership_risk = {\n",
    "    'MORTGAGE': 0,  # 5.83%\n",
    "    'OWN': 1,      # 7.00%\n",
    "    'RENT': 2,     # 8.40%\n",
    "    'NONE': 3,     # 9.50%\n",
    "    'OTHER': 4     # 10.87%\n",
    "}\n",
    "\n",
    "# Credit limit group encoding (already ordered by risk Q5 to Q1)\n",
    "credit_limit_risk = {\n",
    "    'Q5': 0,  # 4.73%\n",
    "    'Q4': 1,  \n",
    "    'Q3': 2,\n",
    "    'Q2': 3,\n",
    "    'Q1': 4   # 9.87%\n",
    "}\n",
    "\n",
    "# Loan purpose risk-based grouping and encoding\n",
    "# First, let's define risk groups based on default rates\n",
    "low_risk_purpose = ['credit_card', 'home_improvement', 'car', 'major_purchase']\n",
    "medium_risk_purpose = ['debt_consolidation', 'medical', 'moving', 'vacation', 'house', 'wedding']\n",
    "high_risk_purpose = ['small_business', 'renewable_energy', 'other']\n",
    "\n",
    "def encode_purpose(purpose):\n",
    "    if purpose in low_risk_purpose:\n",
    "        return 0\n",
    "    elif purpose in medium_risk_purpose:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Apply encodings\n",
    "df['home_ownership_encoded'] = df['home_ownership'].map(home_ownership_risk)\n",
    "df['credit_limit_encoded'] = df['credit_limit_group'].map(credit_limit_risk)\n",
    "df['purpose_encoded'] = df['purpose'].apply(encode_purpose)\n",
    "\n",
    "# Binary encoding for term (36 months = 0, 60 months = 1)\n",
    "df['term_encoded'] = df['term'].apply(lambda x: 0 if '36' in x else 1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "df = df.drop(['home_ownership', 'credit_limit_group', 'purpose', 'term'], axis=1)\n",
    "\n",
    "# Verify encodings\n",
    "print(\"\\nEncoded features check:\")\n",
    "for col in ['home_ownership_encoded', 'credit_limit_encoded', 'purpose_encoded', 'term_encoded']:\n",
    "    print(f\"\\n{col} value counts:\")\n",
    "    print(df[col].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nUpdated dataset shape:\", df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Val-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Train: (132619, 23)\n",
      "Validation: (28419, 23)\n",
      "Test: (28419, 23)\n",
      "\n",
      "Class distribution in splits:\n",
      "Train: bad_flag\n",
      "0.0    0.930704\n",
      "1.0    0.069296\n",
      "Name: proportion, dtype: float64\n",
      "Validation: bad_flag\n",
      "0.0    0.930715\n",
      "1.0    0.069285\n",
      "Name: proportion, dtype: float64\n",
      "Test: bad_flag\n",
      "0.0    0.930715\n",
      "1.0    0.069285\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Train-Val-Test Split\n",
    "# First split into train and temp (test + validation)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['bad_flag'], random_state=42)\n",
    "# Split temp into validation and test\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['bad_flag'], random_state=42)\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Validation: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "# Verify class distribution in splits\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "print(\"Train:\", train_df['bad_flag'].value_counts(normalize=True))\n",
    "print(\"Validation:\", val_df['bad_flag'].value_counts(normalize=True))\n",
    "print(\"Test:\", test_df['bad_flag'].value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations and Feature Engineering\n",
    "1. Log transformation for right-skewed features\n",
    "- annual_inc\n",
    "- loan_amnt\n",
    "\n",
    "2. Standard scaling for normally distributed features:\n",
    "- int_rate\n",
    "- dti\n",
    "- revol_util\n",
    "- bc_util\n",
    "\n",
    "3. Feature Engineering\n",
    "- Composite utilization feature (from revol_util and bc_util)\n",
    "- Income-to-loan ratio (using log transformed values)\n",
    "- Relevant interaction terms based on our EDA insights\n",
    "\n",
    "**Implementation Strategy**:\n",
    "1. Apply log transformations to all splits (no fitting required)\n",
    "2. Fit scalers only on training data, then transform all splits\n",
    "3. Create engineered features using the properly scaled values\n",
    "4. Drop original features that have been transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "Train: (132619, 25)\n",
      "Validation: (28419, 25)\n",
      "Test: (28419, 25)\n",
      "\n",
      "Missing values in final datasets:\n",
      "Train: 0\n",
      "Validation: 0\n",
      "Test: 0\n",
      "\n",
      "Engineered features summary in training set:\n",
      "       composite_util  income_to_loan_ratio   int_rate_dti     risk_score\n",
      "count    1.326190e+05         132619.000000  132619.000000  132619.000000\n",
      "mean     4.671980e-17              1.181601       0.146952      -1.181601\n",
      "std      9.298377e-01              0.080174       1.003699       2.095974\n",
      "min     -2.395327e+00              1.050047      -5.712039      -7.821150\n",
      "25%     -6.408539e-01              1.124076      -0.295388      -2.562022\n",
      "50%      1.410476e-01              1.164254       0.024781      -1.078546\n",
      "75%      7.514079e-01              1.218175       0.532429       0.307960\n",
      "max      4.246996e+00              1.766755       6.099598       5.128489\n"
     ]
    }
   ],
   "source": [
    "# Scaling and Feature Transformations (fit only on training data)\n",
    "\n",
    "# Log transformations first\n",
    "for df_ in [train_df, val_df, test_df]:\n",
    "    df_['log_annual_inc'] = np.log1p(df_['annual_inc'])\n",
    "    df_['log_loan_amt'] = np.log1p(df_['loan_amnt'])\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "scale_features = [\n",
    "    'int_rate',\n",
    "    'dti',\n",
    "    'revol_util',\n",
    "    'bc_util',\n",
    "    'percent_bc_gt_75',\n",
    "    'inq_last_6mths',\n",
    "    'mths_since_recent_inq',\n",
    "    'total_bc_limit',\n",
    "    'tot_hi_cred_lim',\n",
    "    'tot_cur_bal'\n",
    "]\n",
    "\n",
    "# Fit scaler only on training data\n",
    "scaler_dict = {}\n",
    "for feature in scale_features:\n",
    "    scaler_dict[feature] = StandardScaler()\n",
    "    train_df[f'{feature}_scaled'] = scaler_dict[feature].fit_transform(train_df[[feature]])\n",
    "    # Transform validation and test using the scaler fit on training data\n",
    "    val_df[f'{feature}_scaled'] = scaler_dict[feature].transform(val_df[[feature]])\n",
    "    test_df[f'{feature}_scaled'] = scaler_dict[feature].transform(test_df[[feature]])\n",
    "\n",
    "# Feature Engineering (using scaled features)\n",
    "for df_ in [train_df, val_df, test_df]:\n",
    "    # Composite utilization\n",
    "    df_['composite_util'] = (df_['revol_util_scaled'] + df_['bc_util_scaled']) / 2\n",
    "    \n",
    "    # Income to loan ratio using log transformed values\n",
    "    df_['income_to_loan_ratio'] = df_['log_annual_inc'] / df_['log_loan_amt']\n",
    "    \n",
    "    # Interest rate * DTI interaction\n",
    "    df_['int_rate_dti'] = df_['int_rate_scaled'] * df_['dti_scaled']\n",
    "    \n",
    "    # Risk score\n",
    "    df_['risk_score'] = (df_['int_rate_scaled'] + \n",
    "                        df_['dti_scaled'] + \n",
    "                        df_['composite_util'] - \n",
    "                        df_['income_to_loan_ratio'])\n",
    "\n",
    "# Drop original features that have been transformed\n",
    "columns_to_drop = scale_features + ['annual_inc', 'loan_amnt', 'revol_util_scaled', 'bc_util_scaled']\n",
    "train_df = train_df.drop(columns_to_drop, axis=1)\n",
    "val_df = val_df.drop(columns_to_drop, axis=1)\n",
    "test_df = test_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Verify final shapes and features\n",
    "print(\"\\nFinal shapes:\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Validation: {val_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"\\nMissing values in final datasets:\")\n",
    "print(\"Train:\", train_df.isnull().sum().sum())\n",
    "print(\"Validation:\", val_df.isnull().sum().sum())\n",
    "print(\"Test:\", test_df.isnull().sum().sum())\n",
    "\n",
    "# Verify engineered features in training set\n",
    "print(\"\\nEngineered features summary in training set:\")\n",
    "engineered_features = ['composite_util', 'income_to_loan_ratio', 'int_rate_dti', 'risk_score']\n",
    "print(train_df[engineered_features].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strategy for Handling Class Imbalance**:\n",
    "- Let's calculate class weights based on our training data rather than SMOTE or other resampling techniques which can lead to overfitting.\n",
    "- The weights will be used later in the loss function when training the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced class weights:\n",
      "{0.0: 0.537227879995787, 1.0: 7.215397170837868}\n",
      "\n",
      "Class weights tensor:\n",
      "tensor([0.5372, 7.2154])\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights using sklearn's balanced method\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "balanced_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_df['bad_flag']),\n",
    "    y=train_df['bad_flag']\n",
    ")\n",
    "balanced_weights_dict = dict(zip(np.unique(train_df['bad_flag']), balanced_weights))\n",
    "\n",
    "# Convert to PyTorch tensor for later use\n",
    "class_weights_tensor = torch.FloatTensor([balanced_weights_dict[0], balanced_weights_dict[1]])\n",
    "\n",
    "print(\"Balanced class weights:\")\n",
    "print(balanced_weights_dict)\n",
    "print(\"\\nClass weights tensor:\")\n",
    "print(class_weights_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:\n",
    "The class weights look good! The weights reflect the imbalance in our data:\n",
    "- Majority class (0.0): weight ≈ 0.54\n",
    "- Minority class (1.0): weight ≈ 7.22\n",
    "\n",
    "This means the loss function will weigh errors on the minority class (bad loans) about 13.4 times more heavily than errors on the majority class (good loans), which aligns with our class distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tensors and Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 24\n",
      "\n",
      "DataLoader sizes:\n",
      "Training batches: 260\n",
      "Validation batches: 56\n",
      "Test batches: 56\n",
      "\n",
      "Batch shapes:\n",
      "X batch shape: torch.Size([512, 24])\n",
      "y batch shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset and DataLoader\n",
    "\n",
    "# First, let's separate features and target for each split\n",
    "def prepare_features_target(df):\n",
    "   # Separate target\n",
    "   y = df['bad_flag'].values\n",
    "   # Drop target from features\n",
    "   X = df.drop('bad_flag', axis=1).values\n",
    "   return X, y\n",
    "\n",
    "# Prepare data for all splits\n",
    "X_train, y_train = prepare_features_target(train_df)\n",
    "X_val, y_val = prepare_features_target(val_df)\n",
    "X_test, y_test = prepare_features_target(test_df)\n",
    "\n",
    "# Create PyTorch Dataset class\n",
    "class LoanDataset(Dataset):\n",
    "   def __init__(self, X, y):\n",
    "       self.X = torch.FloatTensor(X)\n",
    "       self.y = torch.FloatTensor(y)\n",
    "   \n",
    "   def __len__(self):\n",
    "       return len(self.y)\n",
    "   \n",
    "   def __getitem__(self, idx):\n",
    "       return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = LoanDataset(X_train, y_train)\n",
    "val_dataset = LoanDataset(X_val, y_val)\n",
    "test_dataset = LoanDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 512  # Can be tuned\n",
    "\n",
    "train_loader = DataLoader(\n",
    "   train_dataset, \n",
    "   batch_size=batch_size, \n",
    "   shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "   val_dataset, \n",
    "   batch_size=batch_size, \n",
    "   shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "   test_dataset, \n",
    "   batch_size=batch_size, \n",
    "   shuffle=False\n",
    ")\n",
    "\n",
    "# Verify the dataloaders\n",
    "print(\"Number of features:\", X_train.shape[1])\n",
    "print(\"\\nDataLoader sizes:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Verify a batch\n",
    "X_batch, y_batch = next(iter(train_loader))\n",
    "print(\"\\nBatch shapes:\")\n",
    "print(f\"X batch shape: {X_batch.shape}\")\n",
    "print(f\"y batch shape: {y_batch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e037d88ffd2d50f0c29b82c5dfe65ed58795938fdfc909003e4300710d030f88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
